\documentclass{beamer}
% \documentclass[aspectratio=169]{beamer} % 16:9 widescreen

\usepackage{hyperref}
\usepackage[T1]{fontenc}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{biblatex}
\usepackage{graphicx,pstricks,listings,stackengine}

\addbibresource{refs.bib}

% \author{Santosh Desai, and Matt McAnear, Urvi Mehta}
\author{Santosh Desai, Matt McAnear, Urvi Mehta}
\title{Statistical Efficiency of Birth-Death Tree Parameter Estimation}
\subtitle{Stat 700}
\institute{Department of Statistics, University of Michigan}
\date{\today}

% Remove this if you don't have a custom CNU.sty theme
% \usepackage{CNU}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}


\begin{frame}
\maketitle
\end{frame}


\section{Background}

\begin{frame}{The Phylodeep Package}
    Phylodeep Paper is byt J. Voznica et al \cite{voznica_deep_2022}
    \begin{itemize}
        \item Parameter estimation is difficult and MLE method can be challenging for larger trees.
        \item Major components of the paper to address this
        \begin{itemize}
            \item CBLV representation of trees
            \item Pre-trained neural networks for pre-specified tree types
        \end{itemize}
        \item The goal is to speed up inference
    \end{itemize}
\end{frame}


\begin{frame}{Usage}
    \begin{itemize}
        \item Input: Phylogenetic tree in Newick format
        \item Output: Estimated parameters (e.g., birth rate ($\lambda$), death rate ($\mu$), $R_0$)
        \item Given a tree, Phylodeep selects the pre-trained model with the highest probability
        \item Outputs actually come with uncertainty estimates from a parametric bootstrap
    \end{itemize}
\end{frame}


\section{Methods}

\subsection{MLE}
\begin{frame}{Maximum Likelihood Estimation: Overview}
    \begin{itemize}
        \item \textbf{Goal}: Estimate birth rate ($\lambda$) and death rate ($\mu$) from phylogenetic trees
        \item \textbf{Method}: Numerical optimization of Stadler (2010) likelihood
        \item \textbf{Key Feature}: Works with \textbf{tree data only} - no nucleotide sequences required
        \item \textbf{Implementation}: Uses \texttt{scipy.optimize.minimize} with L-BFGS-B algorithm
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Advantages}:
    \begin{itemize}
        \item Statistically principled (maximum likelihood)
        \item Works on small trees (no minimum tip size requirement)
        \item Provides point estimates with optimization diagnostics
    \end{itemize}
\end{frame}

\begin{frame}{Stadler (2010) Likelihood Formulation}
    For a constant-rate birth-death process with sampling probability $\rho$:
    
    \begin{align*}
        \ell(\lambda, \mu | T, \rho) &= (n-1) \log(\lambda) \\
        &\quad - (\lambda + \mu) T_{\text{total}} \\
        &\quad + n \log(\rho) \\
        &\quad - r \cdot T + \text{penalty terms}
    \end{align*}
    
    where:
    \begin{itemize}
        \item $n$ = number of tips
        \item $T_{\text{total}}$ = total branch length
        \item $T$ = tree height (time from root to present)
        \item $r = \lambda - \mu$ = net diversification rate
        \item $\rho$ = sampling probability
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Key insight}: Survival probability term ($-r \cdot T$) helps identify $\mu$ separately from $\lambda$
\end{frame}

\begin{frame}{MLE Implementation: Tree Statistics Extraction}
    \textbf{Extracted from each tree}:
    \begin{itemize}
        \item Number of tips: $n$
        \item Total branch length: $T_{\text{total}} = \sum_{e \in E} \ell(e)$
        \item Tree height: $T = \max_{v \in V} d(\text{root}, v)$
        \item Branching times: All internal node ages
        \item Mean branch length: $\bar{\ell}$
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Initial parameter estimates} (method-of-moments):
    \begin{align*}
        \hat{r} &= \frac{\log(n)}{T} \\
        \hat{\lambda} &= \frac{n-1}{T_{\text{total}}} \\
        \hat{\mu} &= \max(0.01, \hat{\lambda} - \hat{r})
    \end{align*}
    
    These serve as starting values for numerical optimization.
\end{frame}

\begin{frame}{MLE Implementation: Optimization}
    \textbf{Numerical optimization}:
    \begin{itemize}
        \item \textbf{Algorithm}: L-BFGS-B (bounded optimization)
        \item \textbf{Bounds}: $\lambda \in [0.01, 10.0]$, $\mu \in [0.01, 10.0]$
        \item \textbf{Constraint}: $\mu < \lambda$ (enforced via penalty)
        \item \textbf{Gradient}: Analytical gradient provided for faster convergence
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Gradient components}:
    \begin{align*}
        \frac{\partial \ell}{\partial \lambda} &= -\frac{n-1}{\lambda} + T_{\text{total}} + T \\
        \frac{\partial \ell}{\partial \mu} &= T_{\text{total}} - T + \text{penalty term}
    \end{align*}
    
    \vspace{0.2cm}
    \textbf{Regularization}: Penalty term prevents $\mu$ from collapsing to lower bound in large trees
\end{frame}

\begin{frame}{MLE Implementation: Key Features}
    \textbf{1. Survival Probability Term}
    \begin{itemize}
        \item Includes $-r \cdot T$ term to help identify $\mu$
        \item Accounts for probability of observing $n$ tips given $\lambda$ and $\mu$
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{2. Regularization for Large Trees}
    \begin{itemize}
        \item Penalty: $-0.1 \log(\mu)$ when $\mu < 0.1$ and $n > 10$
        \item Prevents $\mu$ from getting stuck at lower bound (0.01)
        \item Encourages realistic death rate estimates
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{3. Robust Error Handling}
    \begin{itemize}
        \item Checks for invalid trees ($n \leq 1$, $T_{\text{total}} \leq 0$)
        \item Handles optimization failures gracefully
        \item Returns success status and diagnostic information
    \end{itemize}
\end{frame}

\begin{frame}{MLE: Advantages and Limitations}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{✓} Statistically principled (maximum likelihood)
        \item \textcolor{deepgreen}{✓} No minimum tree size requirement
        \item \textcolor{deepgreen}{✓} Works on small trees ($n < 50$)
        \item \textcolor{deepgreen}{✓} Provides optimization diagnostics
        \item \textcolor{deepgreen}{✓} Fast computation (seconds per tree)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{✗} Higher RMSE than PhyloDeep on large trees
        \item \textcolor{deepred}{✗} Point estimates only (no uncertainty quantification)
        \item \textcolor{deepred}{✗} Requires careful initialization for convergence
        \item \textcolor{deepred}{✗} Can struggle with very large trees ($n > 500$)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Best use case}: Small to medium trees where statistical rigor is important
\end{frame}

\subsection{Phylodeep}
\begin{frame}{Phylodeep Methodology}
    SANTOSH TO FILL IN
\end{frame}


\section{Results}

\subsection{RMSE}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/RMSE_vs_tips.png}
        \caption[Figure 1]{Rolling bootstrapped RMSE estimates of $\mu$ and $\lambda$ by tree size and resulting 95\% CI. Window size is 500.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/estimation_error_vs_tips.png}
        \caption[Short Title]{Raw plot of the absolute errors in $\lambda, \mu$ by tree size.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=0.8\linewidth]{images/lambda_vs_mu_estimates.png}
        \caption{Plot of $\lambda$ vs $\mu$ estimates to show distribution of estimates.}
    \end{figure}
\end{frame}

\begin{frame}
    Main takeaways:
    \begin{itemize}
        \item RMSE decreases with increasing tree size for both methods.
        \item MLE can be used on smaller trees - Phylodeep requires a minimum tip size of around 50.
        \item Otherwise, Phylodeep has lower RMSE than MLE, probably due to being trained on 3.9M trees.
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}{References}
    \printbibliography
\end{frame}

\end{document}
