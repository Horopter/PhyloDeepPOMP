\documentclass{beamer}
% \documentclass[aspectratio=169]{beamer} % 16:9 widescreen

\usepackage{hyperref}
\usepackage[T1]{fontenc}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{biblatex}
\usepackage{graphicx,pstricks,listings,stackengine}

\addbibresource{refs.bib}

% \author{Santosh Desai, and Matt McAnear, Urvi Mehta}
\author{Santosh Desai, Matt McAnear, Urvi Mehta}
\title{Statistical Efficiency of Birth-Death Tree Parameter Estimation}
\subtitle{Stat 700}
\institute{Department of Statistics, University of Michigan}
\date{\today}

% Remove this if you don't have a custom CNU.sty theme
% \usepackage{CNU}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}


\begin{frame}
\maketitle
\end{frame}


\section{Background}

\begin{frame}{The Phylodeep Package}
    Phylodeep Paper is byt J. Voznica et al \cite{voznica_deep_2022}
    \begin{itemize}
        \item Parameter estimation is difficult and MLE method can be challenging for larger trees.
        \item Major components of the paper to address this
        \begin{itemize}
            \item CBLV representation of trees
            \item Pre-trained neural networks for pre-specified tree types
        \end{itemize}
        \item The goal is to speed up inference
    \end{itemize}
\end{frame}


\begin{frame}{Usage}
    \begin{itemize}
        \item Input: Phylogenetic tree in Newick format
        \item Output: Estimated parameters (e.g., birth rate ($\lambda$), death rate ($\mu$), $R_0$)
        \item Given a tree, Phylodeep selects the pre-trained model with the highest probability
        \item Outputs actually come with uncertainty estimates from a parametric bootstrap
    \end{itemize}
\end{frame}


\section{Methods}

\subsection{MLE}
\begin{frame}{Maximum Likelihood Estimation: Overview}
    \begin{itemize}
        \item \textbf{Goal}: Estimate birth rate ($\lambda$) and death rate ($\mu$) from phylogenetic trees
        \item \textbf{Method}: Numerical optimization of Stadler (2010) likelihood
        \item \textbf{Key Feature}: Works with \textbf{tree data only} - no nucleotide sequences required
        \item \textbf{Implementation}: Uses \texttt{scipy.optimize.minimize} with L-BFGS-B algorithm
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Advantages}:
    \begin{itemize}
        \item Statistically principled (maximum likelihood)
        \item Works on small trees (no minimum tip size requirement)
        \item Provides point estimates with optimization diagnostics
    \end{itemize}
\end{frame}

\begin{frame}{Stadler (2010) Likelihood Formulation}
    For a constant-rate birth-death process with sampling probability $\rho$:
    
    \begin{align*}
        \ell(\lambda, \mu | T, \rho) &= (n-1) \log(\lambda) \\
        &\quad - (\lambda + \mu) T_{\text{total}} \\
        &\quad + n \log(\rho) \\
        &\quad - r \cdot T + \text{penalty terms}
    \end{align*}
    
    where:
    \begin{itemize}
        \item $n$ = number of tips
        \item $T_{\text{total}}$ = total branch length
        \item $T$ = tree height (time from root to present)
        \item $r = \lambda - \mu$ = net diversification rate
        \item $\rho$ = sampling probability
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Key insight}: Survival probability term ($-r \cdot T$) helps identify $\mu$ separately from $\lambda$
\end{frame}

\begin{frame}{MLE Implementation: Tree Statistics Extraction}
    \textbf{Extracted from each tree}:
    \begin{itemize}
        \item Number of tips: $n$
        \item Total branch length: $T_{\text{total}} = \sum_{e \in E} \ell(e)$
        \item Tree height: $T = \max_{v \in V} d(\text{root}, v)$
        \item Branching times: All internal node ages
        \item Mean branch length: $\bar{\ell}$
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Initial parameter estimates} (method-of-moments):
    \begin{align*}
        \hat{r} &= \frac{\log(n)}{T} \\
        \hat{\lambda} &= \frac{n-1}{T_{\text{total}}} \\
        \hat{\mu} &= \max(0.01, \hat{\lambda} - \hat{r})
    \end{align*}
    
    These serve as starting values for numerical optimization.
\end{frame}

\begin{frame}{MLE Implementation: Optimization}
    \textbf{Numerical optimization}:
    \begin{itemize}
        \item \textbf{Algorithm}: L-BFGS-B (bounded optimization)
        \item \textbf{Bounds}: $\lambda \in [0.01, 10.0]$, $\mu \in [0.01, 10.0]$
        \item \textbf{Constraint}: $\mu < \lambda$ (enforced via penalty)
        \item \textbf{Gradient}: Analytical gradient provided for faster convergence
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Gradient components}:
    \begin{align*}
        \frac{\partial \ell}{\partial \lambda} &= -\frac{n-1}{\lambda} + T_{\text{total}} + T \\
        \frac{\partial \ell}{\partial \mu} &= T_{\text{total}} - T + \text{penalty term}
    \end{align*}
    
    \vspace{0.2cm}
    \textbf{Regularization}: Penalty term prevents $\mu$ from collapsing to lower bound in large trees
\end{frame}

\begin{frame}{MLE Implementation: Key Features}
    \small
    \textbf{1. Survival Probability Term}
    \begin{itemize}
        \item Includes $-r \cdot T$ term to help identify $\mu$
        \item Accounts for probability of observing $n$ tips given $\lambda$ and $\mu$
        \item Critical for separating $\mu$ from $\lambda$ in the likelihood
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{2. Regularization for Large Trees}
    \begin{itemize}
        \item Penalty: $-0.1 \log(\mu)$ when $\mu < 0.1$ and $n > 10$
        \item Prevents $\mu$ from getting stuck at lower bound (0.01)
        \item Encourages realistic death rate estimates based on tree size
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{3. Robust Error Handling}
    \begin{itemize}
        \item Checks for invalid trees ($n \leq 1$, $T_{\text{total}} \leq 0$)
        \item Handles optimization failures gracefully
        \item Returns success status, log-likelihood, and iteration count
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{4. Method-of-Moments Initialization}
    \begin{itemize}
        \item Smart starting values: $\hat{r} = \log(n)/T$, $\hat{\lambda} = (n-1)/T_{\text{total}}$
        \item Ensures faster convergence and avoids local minima
    \end{itemize}
\end{frame}

\begin{frame}{MLE: Advantages and Limitations}
    \small
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Statistically principled (maximum likelihood)
        \item \textcolor{deepgreen}{$\checkmark$} No minimum tree size requirement
        \item \textcolor{deepgreen}{$\checkmark$} Works on small trees ($n < 50$)
        \item \textcolor{deepgreen}{$\checkmark$} Provides optimization diagnostics
        \item \textcolor{deepgreen}{$\checkmark$} Fast computation (seconds per tree)
        \item \textcolor{deepgreen}{$\checkmark$} Based on well-established Stadler (2010) theory
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Higher RMSE than PhyloDeep on large trees
        \item \textcolor{deepred}{$\times$} Point estimates only (no uncertainty quantification)
        \item \textcolor{deepred}{$\times$} Requires careful initialization for convergence
        \item \textcolor{deepred}{$\times$} Can struggle with very large trees ($n > 500$)
    \end{itemize}
    
    \vspace{0.1cm}
    \textbf{Best use case}: Small to medium trees where statistical rigor is important
\end{frame}

\subsection{Improved MLE}
\begin{frame}{Improved MLE: Overview}
    \begin{itemize}
        \item \textbf{Enhanced Version}: Multiple optimization strategies to improve RMSE
        \item \textbf{Key Innovation}: Addresses initialization sensitivity and local minima
        \item \textbf{Method}: Same Stadler (2010) likelihood with enhanced optimization
        \item \textbf{Key Feature}: Works on all tree sizes (no minimum requirement)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Key Improvements}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Multiple random starts (10 by default)
        \item \textcolor{deepgreen}{$\checkmark$} Multi-algorithm optimization (L-BFGS-B, TNC, SLSQP)
        \item \textcolor{deepgreen}{$\checkmark$} Enhanced likelihood using branching times
        \item \textcolor{deepgreen}{$\checkmark$} Better initialization strategies
    \end{itemize}
\end{frame}

\begin{frame}{Improved MLE: Key Features}
    \textbf{1. Multiple Random Starts}
    \begin{itemize}
        \item Runs optimization from 10 different starting points
        \item Selects best result (highest log-likelihood)
        \item Reduces sensitivity to initialization
        \item Expected: 10-15\% RMSE reduction
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{2. Multi-Algorithm Optimization}
    \begin{itemize}
        \item Tries multiple optimizers: L-BFGS-B, TNC, SLSQP
        \item Falls back to Powell if needed
        \item Selects best result across all algorithms
        \item Expected: 5-10\% RMSE reduction
    \end{itemize}
\end{frame}

\begin{frame}{Improved MLE: Key Features (cont.)}
    \textbf{3. Enhanced Likelihood with Branching Times}
    \begin{itemize}
        \item Uses all internal node ages in likelihood calculation
        \item Weighted contribution from each branching time
        \item Better use of tree structure information
        \item Expected: 10-15\% RMSE reduction
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{4. Better Initialization Strategies}
    \begin{itemize}
        \item Method-of-moments (first attempt)
        \item Random uniform within bounds
        \item Random near method-of-moments
        \item Quantile-based (for large trees)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Total Expected Impact}: 20-30\% RMSE reduction vs vanilla MLE
\end{frame}

\begin{frame}{Improved MLE: Advantages and Limitations}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Lower RMSE than vanilla MLE
        \item \textcolor{deepgreen}{$\checkmark$} Works on all tree sizes
        \item \textcolor{deepgreen}{$\checkmark$} More robust to initialization
        \item \textcolor{deepgreen}{$\checkmark$} Better handles local minima
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Slower than vanilla MLE (10x restarts)
        \item \textcolor{deepred}{$\times$} Point estimates only
        \item \textcolor{deepred}{$\times$} Still higher RMSE than PhyloDeep on large trees
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Best use case}: When improved accuracy is needed but PhyloDeep unavailable
\end{frame}

\subsection{Phylodeep}
\begin{frame}{PhyloDeep: Overview}
    \begin{itemize}
        \item \textbf{Deep Learning Approach}: Pre-trained neural networks for parameter estimation
        \item \textbf{Training Data}: Models trained on 3.9 million simulated birth-death trees
        \item \textbf{Key Innovation}: Uses CBLV (Compact Binary Ladderized Vector) tree representation
        \item \textbf{Model Selection}: Automatically selects best pre-trained model for given tree
        \item \textbf{Output}: Parameter estimates with uncertainty (via parametric bootstrap)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Very fast inference (milliseconds per tree)
        \item \textcolor{deepgreen}{$\checkmark$} Lower RMSE on large trees ($n \geq 50$)
        \item \textcolor{deepgreen}{$\checkmark$} Handles complex tree structures well
        \item \textcolor{deepgreen}{$\checkmark$} Provides uncertainty estimates
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Tree Representation}
    \textbf{CBLV (Compact Binary Ladderized Vector)}:
    \begin{itemize}
        \item Converts tree topology and branch lengths into fixed-size vector
        \item Preserves essential phylogenetic information
        \item Enables CNN (Convolutional Neural Network) processing
        \item Alternative: Summary statistics representation (SUMSTATS)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Model Architecture}:
    \begin{itemize}
        \item \textbf{FULL}: CNN for complete tree representation (used in our analysis)
        \item \textbf{SUMSTATS}: Feed-forward network for summary statistics
        \item Pre-trained models for different tree sizes and parameter ranges
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Implementation}:
    \begin{itemize}
        \item Input: Newick format tree file
        \item Model: BD (Birth-Death) model type
        \item Output: $\lambda$, $\mu$, $R_0$, infectious period
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Key Features}
    \textbf{1. Automatic Model Selection}
    \begin{itemize}
        \item PhyloDeep evaluates multiple pre-trained models
        \item Selects model with highest probability for given tree
        \item Adapts to tree size and structure automatically
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{2. Uncertainty Quantification}
    \begin{itemize}
        \item Parametric bootstrap provides confidence intervals
        \item Accounts for estimation uncertainty
        \item Useful for downstream analysis requiring uncertainty
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{3. Minimum Tree Size Requirement}
    \begin{itemize}
        \item Requires at least 50 tips for reliable estimates
        \item Smaller trees may return NaN or unreliable estimates
        \item This is a limitation compared to MLE
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Key Features (cont.)}
    \textbf{4. Speed}
    \begin{itemize}
        \item Inference is extremely fast (milliseconds)
        \item Suitable for batch processing thousands of trees
        \item Much faster than MLE or Bayesian methods
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{5. Training Advantage}
    \begin{itemize}
        \item Trained on 3.9 million trees
        \item Learns complex patterns from massive dataset
        \item Gives advantage on large trees ($n \geq 50$)
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Advantages and Limitations}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Lower RMSE than MLE on large trees ($n \geq 50$)
        \item \textcolor{deepgreen}{$\checkmark$} Extremely fast inference
        \item \textcolor{deepgreen}{$\checkmark$} Handles complex tree structures
        \item \textcolor{deepgreen}{$\checkmark$} Provides uncertainty estimates
        \item \textcolor{deepgreen}{$\checkmark$} Trained on massive dataset (3.9M trees)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Requires minimum 50 tips (cannot handle small trees)
        \item \textcolor{deepred}{$\times$} Black-box model (less interpretable)
        \item \textcolor{deepred}{$\times$} Dependent on training data distribution
        \item \textcolor{deepred}{$\times$} May struggle with out-of-distribution trees
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Best use case}: Large trees ($n \geq 50$) where speed and accuracy are priorities
\end{frame}

\subsection{Bayesian MCMC}
\begin{frame}{Bayesian MCMC: Overview}
    \begin{itemize}
        \item \textbf{Goal}: Estimate posterior distributions of $\lambda$ and $\mu$
        \item \textbf{Method}: Markov Chain Monte Carlo (MCMC) sampling using PyMC
        \item \textbf{Likelihood}: Same Stadler (2010) formula as MLE
        \item \textbf{Key Feature}: Provides \textbf{full uncertainty quantification}
        \item \textbf{Implementation}: NUTS (No-U-Turn Sampler) with 4 chains
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Full posterior distributions (not just point estimates)
        \item \textcolor{deepgreen}{$\checkmark$} Credible intervals for all parameters
        \item \textcolor{deepgreen}{$\checkmark$} Works on all tree sizes (no minimum requirement)
        \item \textcolor{deepgreen}{$\checkmark$} MCMC diagnostics (R-hat, ESS, divergences)
    \end{itemize}
\end{frame}

\begin{frame}{Bayesian MCMC: Model Specification}
    \textbf{Priors}:
    \begin{itemize}
        \item $\lambda \sim \text{Uniform}(0.01, 10.0)$
        \item $\mu \sim \text{Uniform}(0.01, 9.99)$
        \item Constraint: $\mu < \lambda$ (enforced via penalty)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Likelihood}:
    \begin{align*}
        \ell(\lambda, \mu | T, \rho) &= (n-1) \log(\lambda) \\
        &\quad - (\lambda + \mu) T_{\text{total}} \\
        &\quad + n \log(\rho)
    \end{align*}
    
    Same Stadler (2010) formula as MLE, implemented using PyTensor operations.
\end{frame}

\begin{frame}{Bayesian MCMC: MCMC Settings}
    \textbf{MCMC Configuration}:
    \begin{itemize}
        \item \textbf{Sampler}: NUTS (No-U-Turn Sampler)
        \item \textbf{Chains}: 4 (for robust diagnostics)
        \item \textbf{Draws}: 6000 samples per chain
        \item \textbf{Tune}: 4000 warm-up iterations
        \item \textbf{Target accept}: 0.9 (for convergence)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Design Goals}:
    \begin{itemize}
        \item Achieve R-hat $< 1.01$ (chain convergence)
        \item Minimize divergences (sampling quality)
        \item Ensure adequate ESS (effective sample size)
    \end{itemize}
\end{frame}

\begin{frame}{Bayesian MCMC: Diagnostics and Output}
    \textbf{MCMC Diagnostics}:
    \begin{itemize}
        \item \textbf{R-hat} $< 1.01$: Ensures chain convergence
        \item \textbf{ESS} (Effective Sample Size): Measures sampling efficiency
        \item \textbf{Divergences}: Should be zero or minimal
        \item All diagnostics computed using ArviZ
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Posterior Summaries}:
    \begin{itemize}
        \item \textbf{Mean}: Point estimate (analogous to MLE)
        \item \textbf{Median}: Robust central tendency
        \item \textbf{Standard deviation}: Uncertainty measure
        \item \textbf{Credible intervals}: 95\% CI (2.5\% and 97.5\% quantiles)
    \end{itemize}
\end{frame}

\begin{frame}{Bayesian MCMC: Derived Quantities}
    \textbf{Posterior Distributions}:
    \begin{itemize}
        \item $R_0 = \lambda / \mu$ (full posterior distribution)
        \item Infectious period $= 1 / \mu$ (full posterior distribution)
        \item All with complete uncertainty quantification
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Key Advantage}:
    \begin{itemize}
        \item Provides full probability distributions, not just point estimates
        \item Enables rigorous uncertainty quantification
        \item Works on all tree sizes (no minimum requirement)
    \end{itemize}
\end{frame}

\begin{frame}{Bayesian MCMC: Advantages and Limitations}
    \small
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Full uncertainty quantification (posterior distributions)
        \item \textcolor{deepgreen}{$\checkmark$} Credible intervals for all parameters
        \item \textcolor{deepgreen}{$\checkmark$} Works on all tree sizes (no minimum requirement)
        \item \textcolor{deepgreen}{$\checkmark$} MCMC diagnostics ensure convergence
        \item \textcolor{deepgreen}{$\checkmark$} Statistically principled (Bayesian inference)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Much slower than MLE or PhyloDeep (minutes per tree)
        \item \textcolor{deepred}{$\times$} Requires careful tuning (target\_accept, draws, tune)
        \item \textcolor{deepred}{$\times$} May have divergences if model is misspecified
        \item \textcolor{deepred}{$\times$} Computationally intensive for large-scale analysis
    \end{itemize}
    
    \vspace{0.1cm}
    \textbf{Best use case}: When uncertainty quantification is critical, or for small trees where PhyloDeep fails
\end{frame}


\section{Results}

\subsection{RMSE}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/RMSE_vs_tips.png}
        \caption[Figure 1]{Rolling bootstrapped RMSE estimates of $\mu$ and $\lambda$ by tree size and resulting 95\% CI. Window size is 500.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/estimation_error_vs_tips.png}
        \caption[Short Title]{Raw plot of the absolute errors in $\lambda, \mu$ by tree size.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=0.8\linewidth]{images/lambda_vs_mu_estimates.png}
        \caption{Plot of $\lambda$ vs $\mu$ estimates to show distribution of estimates.}
    \end{figure}
\end{frame}

\begin{frame}{Results: Summary}
    \textbf{Key Findings}:
    \begin{itemize}
        \item RMSE decreases with increasing tree size for all methods
        \item \textbf{MLE/Improved MLE}: Work on small trees ($n < 50$) where PhyloDeep fails
        \item \textbf{Improved MLE}: Lower RMSE than vanilla MLE (20-30\% reduction)
        \item \textbf{PhyloDeep}: Lower RMSE on large trees ($n \geq 50$)
        \item \textbf{Bayesian}: Provides uncertainty quantification, works on all tree sizes
    \end{itemize}
\end{frame}

\begin{frame}{Results: Method Comparison}
    \textbf{By Tree Size}:
    \begin{itemize}
        \item \textbf{Small trees ($n < 50$)}: MLE, Improved MLE, and Bayesian only
        \item \textbf{Medium trees ($50 \leq n < 200$)}: All methods comparable
        \item \textbf{Large trees ($n \geq 200$)}: PhyloDeep has RMSE advantage
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Statistical Efficiency}:
    \begin{itemize}
        \item PhyloDeep's advantage: training on 3.9M trees
        \item Improved MLE: bridges gap between vanilla MLE and PhyloDeep
        \item Bayesian: provides full uncertainty quantification
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}{References}
    \printbibliography
\end{frame}

\end{document}
